{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ESCS-CustomPrediction.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPQWwuRP/x+hp3riQQwSuIV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abcvivek/EnglishSpellCheckingSystem/blob/master/ESCS_CustomPrediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3qYvSqnmEUOH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 1.1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SD6XCs9TEAqt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import time\n",
        "from math import ceil\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.python.layers.core import Dense\n",
        "from collections import namedtuple\n",
        "\n",
        "path = '/content/drive/My Drive/clean.txt'\n",
        "     \n",
        "def load_file(path): \n",
        "  input_file = os.path.join(path) \n",
        "  with open(input_file) as file:\n",
        "    File = file.read()\n",
        "  return File\n",
        "       \n",
        "file_content = load_file(path)\n",
        "\n",
        "vocab_to_int = {}\n",
        "count = 0\n",
        "\n",
        "for character in file_content:\n",
        "  if character not in vocab_to_int:\n",
        "    vocab_to_int[character] = count\n",
        "    count += 1\n",
        "\n",
        "codes = ['<PAD>','<EOS>','<GO>']\n",
        "for code in codes:\n",
        "  vocab_to_int[code] = count\n",
        "  count += 1\n",
        "\n",
        "print(\"The vocabulary contains {} characters.\".format(len(vocab_to_int)))\n",
        "print(sorted(vocab_to_int.items()))\n",
        "\n",
        "int_to_vocab = {}\n",
        "for character, value in vocab_to_int.items():\n",
        "    int_to_vocab[value] = character\n",
        "\n",
        "print(int_to_vocab.items())\n",
        "\n",
        "sentences = []\n",
        "for sentence in file_content.splitlines():\n",
        "  sentences.append(sentence)\n",
        "print(\" Dataset contains {} sentences.\".format(len(sentences)))\n",
        "\n",
        "int_sentences = []\n",
        "for sentence in sentences:\n",
        "    int_sentence = []\n",
        "    for character in sentence:\n",
        "        int_sentence.append(vocab_to_int[character])\n",
        "    int_sentences.append(int_sentence)\n",
        "\n",
        "max_length = 250\n",
        "min_length = 30\n",
        "\n",
        "good_sentences = []\n",
        "for sentence in int_sentences:\n",
        "    if len(sentence) <= max_length and len(sentence) >= min_length:\n",
        "        good_sentences.append(sentence)\n",
        "\n",
        "print(\"We will use {} to train and test our model.\".format(len(good_sentences)))\n",
        "\n",
        "training, testing = train_test_split(good_sentences, test_size = 0.10, random_state = 2)\n",
        "testing, validation = train_test_split(testing, test_size = 0.70, random_state = 2)\n",
        "print(\"Number of Training sentences:\", len(training))\n",
        "print(\"Number of Validiation sentences:\", len(validation))\n",
        "print(\"Number of Testing sentences:\", len(testing))\n",
        "\n",
        "training_sorted = []\n",
        "validation_sorted = []\n",
        "testing_sorted = []\n",
        "\n",
        "for i in range(min_length, max_length+1):\n",
        "    for sentence in training:\n",
        "        if len(sentence) == i:\n",
        "            training_sorted.append(sentence)\n",
        "\n",
        "    for sentence in validation:\n",
        "        if len(sentence) == i:\n",
        "            validation_sorted.append(sentence)\n",
        "\n",
        "    for sentence in testing:\n",
        "        if len(sentence) == i:\n",
        "            testing_sorted.append(sentence)\n",
        "\n",
        "letters = ['a','b','c','d','e','f','g','h','i','j','k','l','m','n','o','p','q','r','s','t','u','v','w','x','y','z']\n",
        "\n",
        "def noise_maker(sentence, threshold):\n",
        "\n",
        "    '''Relocate, remove, or add characters to create spelling mistakes''' \n",
        "\n",
        "    noisy_sentence = []\n",
        "    i = 0\n",
        "    while i < len(sentence):\n",
        "        random = np.random.uniform(0,1,1)\n",
        "        # Most characters will be correct since the threshold value is high\n",
        "        if random < threshold:\n",
        "            noisy_sentence.append(sentence[i])\n",
        "        else:\n",
        "            new_random = np.random.uniform(0,1,1)\n",
        "            # ~33% chance characters will swap locations\n",
        "            if new_random > 0.67:\n",
        "                if i == (len(sentence) - 1):\n",
        "                    # If last character in sentence, it will not be typed\n",
        "                    continue\n",
        "                else:\n",
        "                    # if any other character, swap order with following character\n",
        "                    noisy_sentence.append(sentence[i+1])\n",
        "                    noisy_sentence.append(sentence[i])\n",
        "                    i += 1\n",
        "\n",
        "            # ~33% chance an extra lower case letter will be added to the sentence\n",
        "            elif new_random < 0.33:\n",
        "                random_letter = np.random.choice(letters, 1)[0]\n",
        "                noisy_sentence.append(vocab_to_int[random_letter])\n",
        "                noisy_sentence.append(sentence[i])\n",
        "\n",
        "            # ~33% chance a character will not be typed\n",
        "            else:\n",
        "                pass     \n",
        "        i += 1\n",
        "    return noisy_sentence\n",
        "\n",
        "# Check to ensure noise_maker is making mistakes correctly.\n",
        "\n",
        "threshold = 0.9\n",
        "for sentence in training_sorted[:5]:\n",
        "    print(sentence)\n",
        "    print(noise_maker(sentence, threshold))\n",
        "    print()\n",
        "\n",
        "def MakeSentenceReadable(correct):\n",
        "  correct_sentence = \"\"\n",
        "  for i in correct:\n",
        "    if i < 28:\n",
        "      correct_sentence += int_to_vocab[i]\n",
        "  return correct_sentence.strip()\n",
        "\n",
        "epochs = 100\n",
        "batch_size = 64\n",
        "num_layers = 4\n",
        "rnn_size = 512\n",
        "embedding_size = 128\n",
        "learning_rate = 0.0005\n",
        "direction = 2\n",
        "threshold = 0.9\n",
        "keep_probability = 0.8\n",
        "\n",
        "def model_inputs():\n",
        "\n",
        "    with tf.name_scope('inputs'):\n",
        "        # ARGS: data type, shape of the tensor to be fed, name for operation\n",
        "        inputs = tf.placeholder(tf.int32, [None, None], name='inputs')\n",
        "\n",
        "    with tf.name_scope('targets'):\n",
        "        targets = tf.placeholder(tf.int32, [None, None], name='targets')\n",
        "\n",
        "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
        "    inputs_length = tf.placeholder(tf.int32, (None,), name='inputs_length')\n",
        "    targets_length = tf.placeholder(tf.int32, (None,), name='targets_length')\n",
        "\n",
        "    # ARGS: input tensor, name for operation\n",
        "    max_target_length = tf.reduce_max(targets_length, name='max_target_len')\n",
        "\n",
        "    return inputs, targets, keep_prob, inputs_length, targets_length, max_target_length\n",
        "\n",
        "def process_encoding_input(targets, vocab_to_int, batch_size):\n",
        "\n",
        "    with tf.name_scope(\"processing_encoding\"):\n",
        "        ending = tf.strided_slice(targets, [0, 0], [batch_size, -1], [1, 1])\n",
        "        dec_input = tf.concat([tf.fill([batch_size, 1], vocab_to_int['<GO>']), ending], 1)\n",
        "    return dec_input\n",
        "\n",
        "def encoding_layer(rnn_size, sequence_length, num_layers, rnn_inputs, keep_prob, direction):\n",
        "\n",
        "    if direction == 1:\n",
        "        with tf.name_scope(\"RNN_Encoder_Cell_1D\"):\n",
        "            for layer in range(num_layers):\n",
        "                with tf.variable_scope('encoder_{0}'.format(layer)):\n",
        "                    lstm = tf.contrib.rnn.LSTMCell(rnn_size)\n",
        "                    drop = tf.contrib.rnn.DropoutWrapper(lstm, input_keep_prob = keep_prob)\n",
        "                    enc_output, enc_state = tf.nn.dynamic_rnn(drop, rnn_inputs, sequence_length, dtype=tf.float32)\n",
        "            return enc_output, enc_state\n",
        "\n",
        "    if direction == 2:\n",
        "        with tf.name_scope(\"RNN_Encoder_Cell_2D\"):\n",
        "            for layer in range(num_layers):\n",
        "                with tf.variable_scope('encoder_{0}'.format(layer)):\n",
        "                    cell_fw = tf.contrib.rnn.LSTMCell(rnn_size)\n",
        "                    cell_fw = tf.contrib.rnn.DropoutWrapper(cell_fw, input_keep_prob = keep_prob)\n",
        "\n",
        "                    cell_bw = tf.contrib.rnn.LSTMCell(rnn_size)\n",
        "                    cell_bw = tf.contrib.rnn.DropoutWrapper(cell_bw, input_keep_prob = keep_prob)\n",
        "\n",
        "                    enc_output, enc_state = tf.nn.bidirectional_dynamic_rnn(cell_fw, cell_bw, rnn_inputs, sequence_length, dtype=tf.float32)\n",
        "      \n",
        "            # Concat outputs\n",
        "            enc_output = tf.concat(enc_output, 2)\n",
        "\n",
        "            # Use only forwarded state\n",
        "            return enc_output, enc_state[0]\n",
        "\n",
        "def decoding_layer(dec_embed_input, embeddings, enc_output, enc_state, vocab_size, inputs_length, targets_length, max_target_length, rnn_size, vocab_to_int, keep_prob, batch_size, num_layers, direction):\n",
        "\n",
        "    with tf.name_scope(\"RNN_Decoder_Cell\"):\n",
        "        for layer in range(num_layers):\n",
        "            with tf.variable_scope('decoder_{}'.format(layer)):\n",
        "                lstm = tf.contrib.rnn.LSTMCell(rnn_size)\n",
        "                dec_cell = tf.contrib.rnn.DropoutWrapper(lstm, input_keep_prob = keep_prob)\n",
        "\n",
        "    output_layer = Dense(vocab_size, kernel_initializer = tf.truncated_normal_initializer(mean = 0.0, stddev=0.1))\n",
        "\n",
        "    attn_mech = tf.contrib.seq2seq.BahdanauAttention(rnn_size, enc_output, inputs_length, normalize=False, name='BahdanauAttention')\n",
        "\n",
        "    with tf.name_scope(\"Attention_Wrapper\"):\n",
        "        dec_cell = tf.contrib.seq2seq.AttentionWrapper(dec_cell, attn_mech, rnn_size)\n",
        "        \n",
        "    initial_state = dec_cell.zero_state(dtype=tf.float32, batch_size=batch_size)\n",
        "    initial_state = initial_state.clone(cell_state=enc_state)\n",
        "\n",
        "    with tf.variable_scope(\"decode\"):\n",
        "        training_logits = training_decoding_layer(dec_embed_input, targets_length, dec_cell, initial_state, output_layer, vocab_size, max_target_length)\n",
        "    with tf.variable_scope(\"decode\", reuse=True):\n",
        "        inference_logits = inference_decoding_layer(embeddings, vocab_to_int['<GO>'], vocab_to_int['<EOS>'], dec_cell, initial_state, output_layer, max_target_length, batch_size)\n",
        "\n",
        "    return training_logits, inference_logits\n",
        "\n",
        "def training_decoding_layer(dec_embed_input, targets_length, dec_cell, initial_state, output_layer, vocab_size, max_target_length):\n",
        "\n",
        "    with tf.name_scope(\"Training_Decoder\"):\n",
        "        training_helper = tf.contrib.seq2seq.TrainingHelper(inputs=dec_embed_input, sequence_length=targets_length, time_major=False)\n",
        "        training_decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell, training_helper, initial_state, output_layer)\n",
        "        training_logits, _, _ = tf.contrib.seq2seq.dynamic_decode(training_decoder, output_time_major=False, impute_finished=True, maximum_iterations=max_target_length)\n",
        "\n",
        "        return training_logits\n",
        "\n",
        "def inference_decoding_layer(embeddings, start_token, end_token, dec_cell, initial_state, output_layer, max_target_length, batch_size):\n",
        "     \n",
        "     with tf.name_scope(\"Inference_Decoder\"):\n",
        "        start_tokens = tf.tile(tf.constant([start_token], dtype=tf.int32), [batch_size], name='start_tokens')\n",
        "\n",
        "        inference_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(embeddings, start_tokens, end_token)\n",
        "        inference_decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell, inference_helper, initial_state, output_layer)\n",
        "        inference_logits, _, _ = tf.contrib.seq2seq.dynamic_decode(inference_decoder, output_time_major=False, impute_finished=True, maximum_iterations=max_target_length)\n",
        "\n",
        "        return inference_logits\n",
        "\n",
        "def seq2seq_model(inputs, targets, keep_prob, inputs_length, targets_length, max_target_length, vocab_size, rnn_size, num_layers, vocab_to_int, batch_size, embedding_size, direction):\n",
        "\n",
        "    enc_embeddings = tf.Variable(tf.random_uniform([vocab_size, embedding_size], -1, 1))\n",
        "    enc_embed_input = tf.nn.embedding_lookup(enc_embeddings, inputs)\n",
        "    enc_output, enc_state = encoding_layer(rnn_size, inputs_length, num_layers, enc_embed_input, keep_prob, direction)\n",
        "\n",
        "    dec_embeddings = tf.Variable(tf.random_uniform([vocab_size, embedding_size], -1, 1))\n",
        "    dec_input = process_encoding_input(targets, vocab_to_int, batch_size)\n",
        "    dec_embed_input = tf.nn.embedding_lookup(dec_embeddings, dec_input)\n",
        "\n",
        "    training_logits, inference_logits  = decoding_layer(dec_embed_input, dec_embeddings, enc_output, enc_state, vocab_size, inputs_length, targets_length, max_target_length, rnn_size, vocab_to_int, keep_prob, batch_size, num_layers, direction)\n",
        "\n",
        "    return training_logits, inference_logits\n",
        "\n",
        "def build_graph(keep_prob, rnn_size, num_layers, batch_size, learning_rate, embedding_size, direction):\n",
        "\n",
        "    tf.reset_default_graph()\n",
        "\n",
        "    # Load model inputs\n",
        "    inputs, targets, keep_prob, inputs_length, targets_length, max_target_length = model_inputs()\n",
        "\n",
        "    # Create the training and inference logits\n",
        "    training_logits, inference_logits = seq2seq_model(tf.reverse(inputs, [-1]), targets, keep_prob, inputs_length, targets_length, max_target_length, len(vocab_to_int)+1, rnn_size, num_layers, vocab_to_int, batch_size, embedding_size, direction)\n",
        "\n",
        "    # Create tensors for the training logits and inference logits\n",
        "    training_logits = tf.identity(training_logits.rnn_output, 'logits')\n",
        "\n",
        "    with tf.name_scope('predictions'):\n",
        "        predictions = tf.identity(inference_logits.sample_id, name='predictions')\n",
        "        tf.summary.histogram('predictions', predictions)\n",
        "\n",
        "    # Create the weights for sequence_loss\n",
        "    masks = tf.sequence_mask(targets_length, max_target_length, dtype=tf.float32, name='masks')\n",
        "\n",
        "    with tf.name_scope(\"cost\"):\n",
        "        # Loss function\n",
        "        cost = tf.contrib.seq2seq.sequence_loss(training_logits, targets, masks)\n",
        "        tf.summary.scalar('cost', cost)\n",
        "\n",
        "    with tf.name_scope(\"optimze\"):\n",
        "        optimizer = tf.train.AdamOptimizer(learning_rate)\n",
        "\n",
        "        # Gradient Clipping\n",
        "        gradients = optimizer.compute_gradients(cost)\n",
        "        capped_gradients = [(tf.clip_by_value(grad, -5., 5.), var) for grad, var in gradients if grad is not None]\n",
        "        train_op = optimizer.apply_gradients(capped_gradients)\n",
        "\n",
        "\n",
        "    # Merge summaries\n",
        "    merged = tf.summary.merge_all()\n",
        "\n",
        "    # Export the nodes\n",
        "    export_nodes = ['inputs', 'targets', 'keep_prob', 'cost', 'inputs_length', 'targets_length', 'predictions', 'merged', 'train_op','optimizer']\n",
        "    Graph = namedtuple('Graph', export_nodes)\n",
        "    local_dict = locals()\n",
        "    graph = Graph(*[local_dict[each] for each in export_nodes])\n",
        "\n",
        "    return graph\n",
        "\n",
        "def pad_sentence_batch(sentence_batch):\n",
        "    max_sentence = max([len(sentence) for sentence in sentence_batch])\n",
        "    return [sentence + [vocab_to_int['<PAD>']] * (max_sentence - len(sentence)) for sentence in sentence_batch]\n",
        "\n",
        "def get_batches(sentences, batch_size, threshold):\n",
        "\n",
        "    for batch_i in range(0, len(sentences)//batch_size):\n",
        "        start_i = batch_i * batch_size\n",
        "        sentences_batch = sentences[start_i:start_i + batch_size]\n",
        "        \n",
        "        sentences_batch_noisy = []\n",
        "        for sentence in sentences_batch:\n",
        "            sentences_batch_noisy.append(noise_maker(sentence, threshold))\n",
        "\n",
        "        sentences_batch_eos = []\n",
        "        for sentence in sentences_batch:\n",
        "            sentence.append(vocab_to_int['<EOS>'])\n",
        "            sentences_batch_eos.append(sentence)\n",
        "\n",
        "        pad_sentences_batch = np.array(pad_sentence_batch(sentences_batch_eos))\n",
        "        pad_sentences_noisy_batch = np.array(pad_sentence_batch(sentences_batch_noisy))\n",
        "\n",
        "        # Need the lengths for the _lengths parameters\n",
        "\n",
        "        pad_sentences_lengths = []\n",
        "        for sentence in pad_sentences_batch:\n",
        "            pad_sentences_lengths.append(len(sentence))\n",
        "        \n",
        "\n",
        "        pad_sentences_noisy_lengths = []\n",
        "        for sentence in pad_sentences_noisy_batch:\n",
        "            pad_sentences_noisy_lengths.append(len(sentence))\n",
        "\n",
        "        yield pad_sentences_noisy_batch, pad_sentences_batch, pad_sentences_noisy_lengths, pad_sentences_lengths\n",
        "\n",
        "\n",
        "def text_to_ints(text):\n",
        "    return [vocab_to_int[word] for word in text]\n",
        "\n",
        "text_sentence = \"the first days of her existence in th country were vrey hard for dolly\"\n",
        "\n",
        "text = text_to_ints(text_sentence)\n",
        "\n",
        "checkpoint = \"/content/drive/My Drive/Ml-Models/Model-12-E-2.ckpt\"\n",
        "\n",
        "model = build_graph(keep_probability, rnn_size, num_layers, batch_size, learning_rate, embedding_size, direction) \n",
        "\n",
        "with tf.Session() as sess:\n",
        "\n",
        "    # Load saved model\n",
        "    saver = tf.train.Saver()\n",
        "    saver.restore(sess, checkpoint)\n",
        "\n",
        "    #Multiply by batch_size to match the model's input parameters\n",
        "    answer_logits = sess.run(model.predictions, {model.inputs: [text]*batch_size, \n",
        "                                                 model.inputs_length: [len(text)]*batch_size,\n",
        "                                                 model.targets_length: [len(text)+1], \n",
        "                                                 model.keep_prob: [1.0]})[0]\n",
        "\n",
        "answer_logits_sentence = MakeSentenceReadable(answer_logits)\n",
        "print(text_sentence)\n",
        "print()\n",
        "print(answer_logits_sentence)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}