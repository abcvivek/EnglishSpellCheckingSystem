{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EnglishSpellCheckingSystemWithComments.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "17_tqtoJFFhN49isPHe4Nw8HnfEwLuI0F",
      "authorship_tag": "ABX9TyOGKqDYaB4trEphLhj/GT9t",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abcvivek/EnglishSpellCheckingSystem/blob/master/EnglishSpellCheckingSystemWithComments.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dMFyl_XMx9Ir",
        "colab_type": "text"
      },
      "source": [
        "**ENGLISH SPELL CHECKING SYSTEM**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VcokmXezzQdJ",
        "colab_type": "text"
      },
      "source": [
        "- Select the Tensorflow Version 1.x to run the below code "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZmFrI1n-yhOh",
        "colab_type": "code",
        "outputId": "b7160823-ec6c-48a8-e397-b6426e5302b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "%tensorflow_version 1.1"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "`%tensorflow_version` only switches the major version: 1.x or 2.x.\n",
            "You set: `1.1`. This will be interpreted as: `1.x`.\n",
            "\n",
            "\n",
            "TensorFlow 1.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dOSNawZryPmZ",
        "colab_type": "text"
      },
      "source": [
        "**Importing Libraries**\n",
        "- numpy are used for maths calculations\n",
        "- tenserflow is used for building Neural Networks\n",
        "- os will help in managing all the os related stuff\n",
        "- time is used for calculating time difference\n",
        "- ceil is used for rounding off the values\n",
        "- train_test_split will help in dividing dataset into training set and testing set\n",
        "- Dense is used for output layer.....you get a m dimensional vector as output. A dense layer thus is used to change the dimensions of your vector. Mathematically speaking, it applies a rotation, scaling, translation transform to your vector.\n",
        "- Python supports a type of container like dictionaries called “namedtuples()” present in module, “collections“. Like dictionaries they contain keys that are hashed to a particular value. But on contrary, it supports both access from key value and iteration, the functionality that dictionaries lack."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DaBPBtzhzjwk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import time\n",
        "from math import ceil\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.python.layers.core import Dense\n",
        "from collections import namedtuple"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-FJhsn14NYF",
        "colab_type": "text"
      },
      "source": [
        "**Loading of Data**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5AzX-ns4yNK",
        "colab_type": "text"
      },
      "source": [
        "- Path variable contains the dataset path\n",
        "- Used dataset is the cleaned one so there is no step of data cleaning\n",
        "- file_content will hold all the contents of dataset by calling load_file function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u0z6Vfck4Yul",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path = '/content/drive/My Drive/clean.txt'\n",
        "     \n",
        "def load_file(path): \n",
        "  input_file = os.path.join(path) \n",
        "  with open(input_file) as file:\n",
        "    File = file.read()\n",
        "  return File\n",
        "       \n",
        "file_content = load_file(path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wBc9tuGO5rrV",
        "colab_type": "text"
      },
      "source": [
        "**Preprocessing of Data**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4xMKBeQS5v01",
        "colab_type": "text"
      },
      "source": [
        "- vocab_to_int is a python dictionary in this key is character and value is the assigned number\n",
        "- First block of code will get all the unique characters present in the dataset and it will add to vocab_to_int dictionary\n",
        "- Numbers are assigned on the first come first serve basis\n",
        "- Second block will add some special tokens to the vocab_to_int dictionary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7BXmivYC7W_N",
        "colab_type": "text"
      },
      "source": [
        "- **GO** - the same as <start> on the picture below - the first token which is fed to the decoder along with the though vector in order to start generating tokens of the answer\n",
        "- **EOS** - \"end of sentence\" - the same as <end> on the picture below - as soon as decoder generates this token we consider the answer to be complete (you can't use usual punctuation marks for this purpose cause their meaning can be different)\n",
        "- **PAD** - your GPU (or CPU at worst) processes your training data in batches and all the sequences in your batch should have the same length. If the max length of your sequence is 8, your sentence My name is guotong1988 will be padded from either side to fit this length: My name is guotong1988 _pad_ _pad_ _pad_ _pad_\n",
        "\n",
        "(https://cloud.githubusercontent.com/assets/2272790/18410099/1d0a1c1a-7761-11e6-9fe1-bd2e5622b90a.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3YkIAleS5m6e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab_to_int = {}\n",
        "count = 0\n",
        "\n",
        "for character in file_content:\n",
        "  if character not in vocab_to_int:\n",
        "    vocab_to_int[character] = count\n",
        "    count += 1\n",
        "\n",
        "codes = ['<PAD>','<EOS>','<GO>']\n",
        "for code in codes:\n",
        "  vocab_to_int[code] = count\n",
        "  count += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UXqMhqLR55fd",
        "colab_type": "code",
        "outputId": "7edeb7a3-3ff3-4137-ae1d-773267cbfc21",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "print(\"The vocabulary contains {} characters.\".format(len(vocab_to_int)))\n",
        "print(sorted(vocab_to_int.items()))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The vocabulary contains 31 characters.\n",
            "[('\\n', 20), (' ', 4), ('<EOS>', 29), ('<GO>', 30), ('<PAD>', 28), ('a', 6), ('b', 14), ('c', 10), ('d', 18), ('e', 12), ('f', 16), ('g', 24), ('h', 1), ('i', 2), ('j', 21), ('k', 23), ('l', 17), ('m', 22), ('n', 19), ('o', 8), ('p', 15), ('q', 27), ('r', 13), ('s', 3), ('t', 0), ('u', 9), ('v', 11), ('w', 5), ('x', 25), ('y', 7), ('z', 26)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bR7Xnf8B8FK3",
        "colab_type": "text"
      },
      "source": [
        "- Another dictionary to convert integers to their respective characters\n",
        "- Key is Numbers and value is character"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sGvYALSB-ni8",
        "colab_type": "code",
        "outputId": "80ca01ab-c822-4609-e8f4-226870ff1e2d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "int_to_vocab = {}\n",
        "for character, value in vocab_to_int.items():\n",
        "    int_to_vocab[value] = character\n",
        "\n",
        "print(int_to_vocab.items())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dict_items([(0, 't'), (1, 'h'), (2, 'i'), (3, 's'), (4, ' '), (5, 'w'), (6, 'a'), (7, 'y'), (8, 'o'), (9, 'u'), (10, 'c'), (11, 'v'), (12, 'e'), (13, 'r'), (14, 'b'), (15, 'p'), (16, 'f'), (17, 'l'), (18, 'd'), (19, 'n'), (20, '\\n'), (21, 'j'), (22, 'm'), (23, 'k'), (24, 'g'), (25, 'x'), (26, 'z'), (27, 'q'), (28, '<PAD>'), (29, '<EOS>'), (30, '<GO>')])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37FXRFw-8Xpf",
        "colab_type": "text"
      },
      "source": [
        "- From the file content we are converting text into sentences.\n",
        "- When we get a newline in the file we split that into lines"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kSoBAl808Ty0",
        "colab_type": "code",
        "outputId": "2324cdf9-e49f-4778-ab21-786f10e185e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "sentences = []\n",
        "for sentence in file_content.splitlines():\n",
        "  sentences.append(sentence)\n",
        "print(\" Dataset contains {} sentences.\".format(len(sentences)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Dataset contains 1232681 sentences.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tXlPW6Dn83vn",
        "colab_type": "text"
      },
      "source": [
        "- As the character cannot be fed to seq2seq model we convert every sentence into numbered sentence.\n",
        "- Conversion of sentences happen using the vocab_to_int dictionary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "An_rcATf82oa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "int_sentences = []\n",
        "for sentence in sentences:\n",
        "    int_sentence = []\n",
        "    for character in sentence:\n",
        "        int_sentence.append(vocab_to_int[character])\n",
        "    int_sentences.append(int_sentence)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bPB-lS-w9QO9",
        "colab_type": "text"
      },
      "source": [
        "- We are limiting the training sentences by keeping max_length and min_length\n",
        "- with this filtering sentence with less than 30 characters and more than 250 characters are discared for training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "egVmlKvW9K2O",
        "colab_type": "code",
        "outputId": "83922109-35b3-4833-db92-5cfc56c0ed04",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "max_length = 250\n",
        "min_length = 30\n",
        "\n",
        "good_sentences = []\n",
        "for sentence in int_sentences:\n",
        "    if len(sentence) <= max_length and len(sentence) >= min_length:\n",
        "        good_sentences.append(sentence)\n",
        "\n",
        "print(\"We will use {} to train and test our model.\".format(len(good_sentences)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "We will use 1017165 to train and test our model.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gaTEjSu_93YG",
        "colab_type": "text"
      },
      "source": [
        "- Here the set of sentences is divided into three parts.They are\n",
        "- **Training Sentence** = used for training the model.\n",
        "- **Validation Sentence** = used for validating how well the training has been done on the model.\n",
        "- **Testing sentence** = Once the model is saved and finalized, these sentences can be used for testing the model for accuracy.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IPtu277x9vL_",
        "colab_type": "code",
        "outputId": "19289c2c-22b9-4eac-f3a7-de3fb3e59f24",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "training, testing = train_test_split(good_sentences, test_size = 0.10, random_state = 2)\n",
        "testing, validation = train_test_split(testing, test_size = 0.70, random_state = 2)\n",
        "print(\"Number of Training sentences:\", len(training))\n",
        "print(\"Number of Validiation sentences:\", len(validation))\n",
        "print(\"Number of Testing sentences:\", len(testing))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of Training sentences: 915448\n",
            "Number of Validiation sentences: 71202\n",
            "Number of Testing sentences: 30515\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XM9jupdV-yHS",
        "colab_type": "text"
      },
      "source": [
        "- Sort all the category sentences by length to reduce padding, which will allow the model to train faster"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4WIclU5u-tOu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "training_sorted = []\n",
        "validation_sorted = []\n",
        "testing_sorted = []\n",
        "\n",
        "for i in range(min_length, max_length+1):\n",
        "    for sentence in training:\n",
        "        if len(sentence) == i:\n",
        "            training_sorted.append(sentence)\n",
        "\n",
        "    for sentence in validation:\n",
        "        if len(sentence) == i:\n",
        "            validation_sorted.append(sentence)\n",
        "\n",
        "    for sentence in testing:\n",
        "        if len(sentence) == i:\n",
        "            testing_sorted.append(sentence)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L57zRCEw_H-V",
        "colab_type": "text"
      },
      "source": [
        "- This method helps in adding noise to the sentences which are correct.\n",
        "- noise_maker will create noises based on the threshold value. If the threshold value is 100 all the sentences are kept as it is. In our case threshold can be around 90 to 95.\n",
        "- noise_maker creates 3 types of mistakes based on randomness. They are 1. Swap the character locations 2. Remove the character  3. Addition of any random lower case letter"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iRYCb_HG-_xm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "letters = ['a','b','c','d','e','f','g','h','i','j','k','l','m','n','o','p','q','r','s','t','u','v','w','x','y','z']\n",
        "\n",
        "def noise_maker(sentence, threshold):\n",
        "\n",
        "    '''Relocate, remove, or add characters to create spelling mistakes''' \n",
        "\n",
        "    noisy_sentence = []\n",
        "    i = 0\n",
        "    while i < len(sentence):\n",
        "        random = np.random.uniform(0,1,1)\n",
        "        # Most characters will be correct since the threshold value is high\n",
        "        if random < threshold:\n",
        "            noisy_sentence.append(sentence[i])\n",
        "        else:\n",
        "            new_random = np.random.uniform(0,1,1)\n",
        "            # ~33% chance characters will swap locations\n",
        "            if new_random > 0.67:\n",
        "                if i == (len(sentence) - 1):\n",
        "                    # If last character in sentence, it will not be typed\n",
        "                    continue\n",
        "                else:\n",
        "                    # if any other character, swap order with following character\n",
        "                    noisy_sentence.append(sentence[i+1])\n",
        "                    noisy_sentence.append(sentence[i])\n",
        "                    i += 1\n",
        "\n",
        "            # ~33% chance an extra lower case letter will be added to the sentence\n",
        "            elif new_random < 0.33:\n",
        "                random_letter = np.random.choice(letters, 1)[0]\n",
        "                noisy_sentence.append(vocab_to_int[random_letter])\n",
        "                noisy_sentence.append(sentence[i])\n",
        "\n",
        "            # ~33% chance a character will not be typed\n",
        "            else:\n",
        "                pass     \n",
        "        i += 1\n",
        "    return noisy_sentence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NYucEPoC__kj",
        "colab_type": "text"
      },
      "source": [
        "- This is a utility method to verify whether the noise_maker is making the desired mistakes or not "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZAywxim9_7Sk",
        "colab_type": "code",
        "outputId": "3a7f8029-34e3-495d-a130-be4c776804bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "# Check to ensure noise_maker is making mistakes correctly.\n",
        "\n",
        "threshold = 0.9\n",
        "for sentence in training_sorted[:5]:\n",
        "    print(sentence)\n",
        "    print(noise_maker(sentence, threshold))\n",
        "    print()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[6, 4, 5, 6, 13, 19, 2, 19, 24, 4, 5, 6, 3, 4, 24, 2, 11, 12, 19, 4, 14, 7, 4, 2, 3, 6, 6, 10, 4, 22]\n",
            "[6, 4, 5, 6, 13, 19, 2, 19, 24, 8, 4, 5, 6, 3, 4, 24, 2, 11, 12, 19, 4, 14, 7, 4, 2, 3, 6, 6, 10, 7, 4, 22]\n",
            "\n",
            "[6, 4, 16, 12, 5, 4, 15, 12, 0, 13, 8, 17, 4, 14, 8, 22, 14, 3, 4, 5, 12, 13, 12, 4, 0, 1, 13, 8, 5, 19]\n",
            "[6, 4, 16, 5, 12, 4, 15, 12, 0, 8, 17, 4, 14, 8, 22, 14, 3, 4, 5, 12, 13, 12, 4, 0, 1, 8, 13, 5, 19]\n",
            "\n",
            "[0, 1, 12, 4, 13, 12, 3, 0, 4, 8, 16, 4, 9, 3, 4, 19, 12, 12, 18, 4, 12, 6, 10, 1, 4, 8, 0, 1, 12, 13]\n",
            "[0, 1, 12, 4, 13, 12, 3, 0, 4, 8, 16, 4, 9, 3, 4, 19, 12, 12, 18, 4, 12, 6, 10, 1, 4, 8, 0, 12, 13]\n",
            "\n",
            "[3, 8, 22, 12, 4, 3, 0, 9, 22, 14, 17, 12, 4, 8, 11, 12, 13, 4, 6, 4, 16, 6, 17, 3, 12, 4, 1, 8, 15, 12]\n",
            "[3, 22, 12, 4, 18, 3, 9, 0, 22, 14, 17, 12, 4, 8, 11, 12, 13, 4, 15, 6, 4, 16, 6, 17, 3, 12, 4, 1, 8, 12, 15]\n",
            "\n",
            "[3, 2, 19, 10, 12, 4, 0, 1, 12, 4, 22, 6, 13, 23, 12, 0, 4, 5, 2, 17, 17, 4, 14, 12, 4, 24, 2, 11, 12, 19]\n",
            "[3, 2, 19, 10, 12, 4, 0, 1, 12, 4, 22, 6, 13, 23, 0, 4, 5, 11, 2, 17, 17, 4, 14, 12, 4, 24, 2, 11, 12]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xonXKB5LANjV",
        "colab_type": "text"
      },
      "source": [
        "- This is also a utility method which is used for making the sentence readable by converting from numbers to their corresponding characters\n",
        "- int_to_vaocab dictionary is used here for conversion.\n",
        "- this method will also remove all unnecessary prediction predicted by the model ( this will increase the accuracy of the model )"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C6zQMZ5dALBW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def MakeSentenceReadable(correct):\n",
        "  correct_sentence = \"\"\n",
        "  for i in correct:\n",
        "    if i < 28:\n",
        "      correct_sentence += int_to_vocab[i]\n",
        "  return correct_sentence.strip()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Zd3zOHBoVPA",
        "colab_type": "text"
      },
      "source": [
        "**Hyper Parameters**\n",
        "\n",
        "- Below are the set of parameters which is used for tuining the model. In this project we are using trail and error method to find the right combination of values to get a model with the good amount of accuracy.\n",
        "- Epochs = A complete cycle of training the model with the whole data and validating the model.\n",
        "- Batch Size = Training is done using batches so at a time we are taking x sentences from dataset. so that x will be our batch size. \n",
        "- RNN layers = Number of rnn layers to be present for the model\n",
        "- Embedding Size = with the given value we construct a array and this array is passed as the first input to the encoder.\n",
        "- Learning Rate = The rate at which model should learn the behaviour of data.\n",
        "- Direction = This used for choosing the direction in encoder layer.\n",
        "- Threshold = Rate at which error has to be made in the correct sentence.\n",
        "- Keep Probability = Value ranges from 0 to 1. 1 means consider all the input layers and 0 means leave all the input layers which will not give any result. This is used for overfitting problem so generally common values are 0.5 to 0.8."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LGrarRZIoPiq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epochs = 100\n",
        "batch_size = 64\n",
        "num_layers = 4\n",
        "rnn_size = 512\n",
        "embedding_size = 128\n",
        "learning_rate = 0.0005\n",
        "direction = 2\n",
        "threshold = 0.9\n",
        "keep_probability = 0.8 "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YzwUNMahA0Gt",
        "colab_type": "text"
      },
      "source": [
        "**Building the Model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ketQBkfV2V35",
        "colab_type": "text"
      },
      "source": [
        "- This method contains necessary pipe kind of structure for model to feed the data.\n",
        "- Here every placeholder acts as a pipe to the model.\n",
        "- tf.name_scope = usually used to group some variables together in an operation.\n",
        "- tf.placeholder = this help in declaring the pipes.\n",
        "- [None,None] = declared as 2d-array with any size.\n",
        "- (None) = 1d-array with any size.\n",
        "- pipes without array is declared as a normal variable.\n",
        "- name attribute is used to identify the placeholders.\n",
        "- tf.reduce_max will take array as an input and returns the maximum value present in that array."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v4TAbIEh2Th1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def model_inputs():\n",
        "\n",
        "    with tf.name_scope('inputs'):\n",
        "        # ARGS: data type, shape of the tensor to be fed, name for operation\n",
        "        inputs = tf.placeholder(tf.int32, [None, None], name='inputs')\n",
        "\n",
        "    with tf.name_scope('targets'):\n",
        "        targets = tf.placeholder(tf.int32, [None, None], name='targets')\n",
        "\n",
        "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
        "    inputs_length = tf.placeholder(tf.int32, (None,), name='inputs_length')\n",
        "    targets_length = tf.placeholder(tf.int32, (None,), name='targets_length')\n",
        "\n",
        "    # ARGS: input tensor, name for operation\n",
        "    max_target_length = tf.reduce_max(targets_length, name='max_target_len')\n",
        "\n",
        "    return inputs, targets, keep_prob, inputs_length, targets_length, max_target_length"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E4UvxdHh71nG",
        "colab_type": "text"
      },
      "source": [
        "- This method will remove last column from the array and it will add the GO token column to the start of the array\n",
        "- tf.strided_slice is used for removing the last column from the given array.\n",
        "- tf.concat is used for adding GO token."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wGVT8mf67mz5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def process_encoding_input(targets, vocab_to_int, batch_size):\n",
        "\n",
        "    with tf.name_scope(\"processing_encoding\"):\n",
        "        ending = tf.strided_slice(targets, [0, 0], [batch_size, -1], [1, 1])\n",
        "        dec_input = tf.concat([tf.fill([batch_size, 1], vocab_to_int['<GO>']), ending], 1)\n",
        "    return dec_input"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pi9-VRGiCeNF",
        "colab_type": "text"
      },
      "source": [
        "- tf.name_scope creates namespace for operators in the default graph.\n",
        "- tf.variable_scope creates namespace for both variables and operators in the default graph.\n",
        "- For this we use Bi-directional RNN with forward and backward direction.\n",
        "- LSTM's are special kind of RNN. ( Simple version of RNN is rarely used, its more advanced version i.e. LSTM or GRU are used. This is because RNN suffers from the problem of vanishing gradient )\n",
        "- Here we have created backward and forward nodes with given no of rnn_size\n",
        "- So 1 layer consists of backward and forward nodes. If we have multiple layers then in the same way that many layers will have same forward and backward structure.\n",
        "- We use Dropout wrapper to remove random nodes inorder to not to over train the model on the given data.\n",
        "- As we are using bi-directional rnn we are concatenating the outputs.\n",
        "- tf.nn.bidirectional_dynamic_rnn combines our embedding layer and RNN layers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EwJ-kjfo8Svy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def encoding_layer(rnn_size, sequence_length, num_layers, rnn_inputs, keep_prob, direction):\n",
        "\n",
        "    if direction == 1:\n",
        "        with tf.name_scope(\"RNN_Encoder_Cell_1D\"):\n",
        "            for layer in range(num_layers):\n",
        "                with tf.variable_scope('encoder_{0}'.format(layer)):\n",
        "                    lstm = tf.contrib.rnn.LSTMCell(rnn_size)\n",
        "                    drop = tf.contrib.rnn.DropoutWrapper(lstm, input_keep_prob = keep_prob)\n",
        "                    enc_output, enc_state = tf.nn.dynamic_rnn(drop, rnn_inputs, sequence_length, dtype=tf.float32)\n",
        "            return enc_output, enc_state\n",
        "\n",
        "    if direction == 2:\n",
        "        with tf.name_scope(\"RNN_Encoder_Cell_2D\"):\n",
        "            for layer in range(num_layers):\n",
        "                with tf.variable_scope('encoder_{0}'.format(layer)):\n",
        "                    cell_fw = tf.contrib.rnn.LSTMCell(rnn_size)\n",
        "                    cell_fw = tf.contrib.rnn.DropoutWrapper(cell_fw, input_keep_prob = keep_prob)\n",
        "\n",
        "                    cell_bw = tf.contrib.rnn.LSTMCell(rnn_size)\n",
        "                    cell_bw = tf.contrib.rnn.DropoutWrapper(cell_bw, input_keep_prob = keep_prob)\n",
        "\n",
        "                    enc_output, enc_state = tf.nn.bidirectional_dynamic_rnn(cell_fw, cell_bw, rnn_inputs, sequence_length, dtype=tf.float32)\n",
        "      \n",
        "            # Concat outputs\n",
        "            enc_output = tf.concat(enc_output, 2)\n",
        "\n",
        "            # Use only forwarded state\n",
        "            return enc_output, enc_state[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mCg8G_sCmuAo",
        "colab_type": "text"
      },
      "source": [
        "- In the first block we are creating the LSTM Decoding layers with dropout wrapper\n",
        "- No of layers depends on the value which is given for the num_layers.\n",
        "- Output layer is used as final prediction layer where activation functions are used to filter out the unwanted predictions.\n",
        "- In this project we are using BahdanauAttention which will help in getting the write character for the spelling correction.\n",
        "- Go through this link as it will help in understanding attention mechanism better.  https://blog.floydhub.com/attention-mechanism/\n",
        "- In the next line we are wrapping the attention mechanism to the decoder layer\n",
        "- intial_state will hold the output of encoder and this will be used as the input for decoder layer.\n",
        "- We are calling training layer used for training and inference layer used for prediction\n",
        "- For more understanding on training and inferencing https://towardsdatascience.com/seq2seq-model-in-tensorflow-ec0c557e560f "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kOZ2jtVRVsTa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def decoding_layer(dec_embed_input, embeddings, enc_output, enc_state, vocab_size, inputs_length, targets_length, max_target_length, rnn_size, vocab_to_int, keep_prob, batch_size, num_layers, direction):\n",
        "\n",
        "    with tf.name_scope(\"RNN_Decoder_Cell\"):\n",
        "        for layer in range(num_layers):\n",
        "            with tf.variable_scope('decoder_{}'.format(layer)):\n",
        "                lstm = tf.contrib.rnn.LSTMCell(rnn_size)\n",
        "                dec_cell = tf.contrib.rnn.DropoutWrapper(lstm, input_keep_prob = keep_prob)\n",
        "\n",
        "    output_layer = Dense(vocab_size, kernel_initializer = tf.truncated_normal_initializer(mean = 0.0, stddev=0.1))\n",
        "\n",
        "    attn_mech = tf.contrib.seq2seq.BahdanauAttention(rnn_size, enc_output, inputs_length, normalize=False, name='BahdanauAttention')\n",
        "\n",
        "    with tf.name_scope(\"Attention_Wrapper\"):\n",
        "        dec_cell = tf.contrib.seq2seq.AttentionWrapper(dec_cell, attn_mech, rnn_size)\n",
        "        \n",
        "    initial_state = dec_cell.zero_state(dtype=tf.float32, batch_size=batch_size)\n",
        "    initial_state = initial_state.clone(cell_state=enc_state)\n",
        "\n",
        "    with tf.variable_scope(\"decode\"):\n",
        "        training_logits = training_decoding_layer(dec_embed_input, targets_length, dec_cell, initial_state, output_layer, vocab_size, max_target_length)\n",
        "    with tf.variable_scope(\"decode\", reuse=True):\n",
        "        inference_logits = inference_decoding_layer(embeddings, vocab_to_int['<GO>'], vocab_to_int['<EOS>'], dec_cell, initial_state, output_layer, max_target_length, batch_size)\n",
        "\n",
        "    return training_logits, inference_logits"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Lu_QF3z-2as",
        "colab_type": "text"
      },
      "source": [
        "-  TrainingHelper is where we pass the embeded input. As the name indicates, this is only a helper instance. This instance should be delivered to the BasicDecoder, which is the actual process of building the decoder model.\n",
        "- BasicDecoder builds the decoder model. It means it connects the RNN layer(s) on the decoder side and the input prepared by TrainingHelper.\n",
        "- Dynamic Decode will help in training the data and settiing the appropriate weights.\n",
        "- time_major attributes=False says that training will happen based on batches not based on time.\n",
        "- impute_finished=True will make sure to correctly save the batch data and the corresponding weights. This process slow downs the training but it will help in getting the accuracte result in the end.\n",
        "- In python if we specify _ as a return variable, then that variable is called as throwaway variable."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YsLZhXQ08-XN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def training_decoding_layer(dec_embed_input, targets_length, dec_cell, initial_state, output_layer, vocab_size, max_target_length):\n",
        "\n",
        "    with tf.name_scope(\"Training_Decoder\"):\n",
        "        training_helper = tf.contrib.seq2seq.TrainingHelper(inputs=dec_embed_input, sequence_length=targets_length, time_major=False)\n",
        "        training_decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell, training_helper, initial_state, output_layer)\n",
        "        training_logits, _, _ = tf.contrib.seq2seq.dynamic_decode(training_decoder, output_time_major=False, impute_finished=True, maximum_iterations=max_target_length)\n",
        "\n",
        "        return training_logits"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NlnsVv16NFDc",
        "colab_type": "text"
      },
      "source": [
        "- GreedyEmbeddingHelper asks to give the start_tokens for the same amount as the batch size and end_token. Bascially these tokens are used for starting the prediction and ending the prediction.\n",
        "- All the functions are same as of training decoding layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rA4TP7UQLv00",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def inference_decoding_layer(embeddings, start_token, end_token, dec_cell, initial_state, output_layer, max_target_length, batch_size):\n",
        "     \n",
        "     with tf.name_scope(\"Inference_Decoder\"):\n",
        "        start_tokens = tf.tile(tf.constant([start_token], dtype=tf.int32), [batch_size], name='start_tokens')\n",
        "\n",
        "        inference_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(embeddings, start_tokens, end_token)\n",
        "        inference_decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell, inference_helper, initial_state, output_layer)\n",
        "        inference_logits, _, _ = tf.contrib.seq2seq.dynamic_decode(inference_decoder, output_time_major=False, impute_finished=True, maximum_iterations=max_target_length)\n",
        "\n",
        "        return inference_logits"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3kicGIGMOTPW",
        "colab_type": "text"
      },
      "source": [
        "- This is the main seq2seq_model function which connets all the above functions to build seq2seq model.\n",
        "- First line enc_embeddings is a tensor of shape [31,128]. Basicaly tensor is like array only so for simplicity we can choose enc_embeddings as 2D-Array where we have 31 rowa and 128 columns. whole array is being filled with random numbers from -1 to 1 ( Don't know exactly why -1 to 1 is used but may be it will help in activation function decision )\n",
        "- enc_embed_input will contain all the inputs embeded value. For example a will be represented as a row with 128 values in that row.\n",
        "- This input is passed to the encoding layer. This input is also commonly known as emmbedding layer.\n",
        "- In the same for decoder also we create the embbedded input\n",
        "- Finally from decoding_layer we get training output and inference output."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o0tppAF8MCjt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def seq2seq_model(inputs, targets, keep_prob, inputs_length, targets_length, max_target_length, vocab_size, rnn_size, num_layers, vocab_to_int, batch_size, embedding_size, direction):\n",
        "\n",
        "    enc_embeddings = tf.Variable(tf.random_uniform([vocab_size, embedding_size], -1, 1))\n",
        "    enc_embed_input = tf.nn.embedding_lookup(enc_embeddings, inputs)\n",
        "    enc_output, enc_state = encoding_layer(rnn_size, inputs_length, num_layers, enc_embed_input, keep_prob, direction)\n",
        "\n",
        "    dec_embeddings = tf.Variable(tf.random_uniform([vocab_size, embedding_size], -1, 1))\n",
        "    dec_input = process_encoding_input(targets, vocab_to_int, batch_size)\n",
        "    dec_embed_input = tf.nn.embedding_lookup(dec_embeddings, dec_input)\n",
        "\n",
        "    training_logits, inference_logits  = decoding_layer(dec_embed_input, dec_embeddings, enc_output, enc_state, vocab_size, inputs_length, targets_length, max_target_length, rnn_size, vocab_to_int, keep_prob, batch_size, num_layers, direction)\n",
        "\n",
        "    return training_logits, inference_logits"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qqhJ8T7Iqwyz",
        "colab_type": "text"
      },
      "source": [
        "- seq2seq model is like a machine is built. But to run the machine we need power. So this power for seq2seq model is defining the graph.\n",
        "- Defining the graph will allow us to perform different type of operation on the model and how those operatins are carried out.\n",
        "- First we are making sure that the graph we are going to construct is with the defualt config set by the tensorflow.\n",
        "- Next we will load all the data's which are required for the model and we pass those data using the placeholder ( Pipe ) to the model.\n",
        "- Pass all the necessary inputs to seq2seq model.\n",
        "- training_logits will have trained outputs and using this we are finding the cost or loss by comparing the real target.\n",
        "- Loss is find using the sequence loss ( We have a max prediction length if the real prediction increases the length then we calculate that extra length and added to loss )\n",
        "- We have used Adam Optimizer for learning. Adam = Adaptive Movement Estimation\n",
        "- For Adam optimizer we use gradient clipping to overcome vanishing and exploding gradient descent.\n",
        "- tf.summary.merge_all() will merge all the available summary's and can be used to visualize it in tensorboard.\n",
        "- All the necessary properties of the graph are made as namedtuple. To understand namedtuple https://www.geeksforgeeks.org/namedtuple-in-python/\n",
        "- For each batch we are exporting the values which are declared in the namedtuple."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vjg6iMUnRnjF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_graph(keep_prob, rnn_size, num_layers, batch_size, learning_rate, embedding_size, direction):\n",
        "\n",
        "    tf.reset_default_graph()\n",
        "\n",
        "    # Load model inputs\n",
        "    inputs, targets, keep_prob, inputs_length, targets_length, max_target_length = model_inputs()\n",
        "\n",
        "    # Create the training and inference logits\n",
        "    training_logits, inference_logits = seq2seq_model(tf.reverse(inputs, [-1]), targets, keep_prob, inputs_length, targets_length, max_target_length, len(vocab_to_int)+1, rnn_size, num_layers, vocab_to_int, batch_size, embedding_size, direction)\n",
        "\n",
        "    # Create tensors for the training logits and inference logits\n",
        "    training_logits = tf.identity(training_logits.rnn_output, 'logits')\n",
        "\n",
        "    with tf.name_scope('predictions'):\n",
        "        predictions = tf.identity(inference_logits.sample_id, name='predictions')\n",
        "        tf.summary.histogram('predictions', predictions)\n",
        "\n",
        "    # Create the weights for sequence_loss\n",
        "    masks = tf.sequence_mask(targets_length, max_target_length, dtype=tf.float32, name='masks')\n",
        "\n",
        "    with tf.name_scope(\"cost\"):\n",
        "        # Loss function\n",
        "        cost = tf.contrib.seq2seq.sequence_loss(training_logits, targets, masks)\n",
        "        tf.summary.scalar('cost', cost)\n",
        "\n",
        "    with tf.name_scope(\"optimze\"):\n",
        "        optimizer = tf.train.AdamOptimizer(learning_rate)\n",
        "\n",
        "        # Gradient Clipping\n",
        "        gradients = optimizer.compute_gradients(cost)\n",
        "        capped_gradients = [(tf.clip_by_value(grad, -5., 5.), var) for grad, var in gradients if grad is not None]\n",
        "        train_op = optimizer.apply_gradients(capped_gradients)\n",
        "\n",
        "\n",
        "    # Merge summaries\n",
        "    merged = tf.summary.merge_all()\n",
        "\n",
        "    # Export the nodes\n",
        "    export_nodes = ['inputs', 'targets', 'keep_prob', 'cost', 'inputs_length', 'targets_length', 'predictions', 'merged', 'train_op','optimizer']\n",
        "    Graph = namedtuple('Graph', export_nodes)\n",
        "    local_dict = locals()\n",
        "    graph = Graph(*[local_dict[each] for each in export_nodes])\n",
        "\n",
        "    return graph"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yga0eUPuCuPt",
        "colab_type": "text"
      },
      "source": [
        "- If in a batch of sentence every sentence has the same length then there is no need of padding.\n",
        "- If the batch has diffrent lengths of sentence then we add PAD token to the sentence which has smaller length.\n",
        "- Padding is required for better training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7bz5iNy_Chwz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pad_sentence_batch(sentence_batch):\n",
        "    max_sentence = max([len(sentence) for sentence in sentence_batch])\n",
        "    return [sentence + [vocab_to_int['<PAD>']] * (max_sentence - len(sentence)) for sentence in sentence_batch]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hRW4iM9sGKxh",
        "colab_type": "text"
      },
      "source": [
        "- This method will help in getting the sentences in each batch depends on the batch size.\n",
        "- For every iteration it will return the correct set of sentence and the noisy sentence and the lengths of the sentence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ii75XB91D11c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_batches(sentences, batch_size, threshold):\n",
        "\n",
        "    for batch_i in range(0, len(sentences)//batch_size):\n",
        "        start_i = batch_i * batch_size\n",
        "        sentences_batch = sentences[start_i:start_i + batch_size]\n",
        "        \n",
        "        sentences_batch_noisy = []\n",
        "        for sentence in sentences_batch:\n",
        "            sentences_batch_noisy.append(noise_maker(sentence, threshold))\n",
        "\n",
        "        sentences_batch_eos = []\n",
        "        for sentence in sentences_batch:\n",
        "            sentence.append(vocab_to_int['<EOS>'])\n",
        "            sentences_batch_eos.append(sentence)\n",
        "\n",
        "        pad_sentences_batch = np.array(pad_sentence_batch(sentences_batch_eos))\n",
        "        pad_sentences_noisy_batch = np.array(pad_sentence_batch(sentences_batch_noisy))\n",
        "\n",
        "        # Need the lengths for the _lengths parameters\n",
        "\n",
        "        pad_sentences_lengths = []\n",
        "        for sentence in pad_sentences_batch:\n",
        "            pad_sentences_lengths.append(len(sentence))\n",
        "        \n",
        "\n",
        "        pad_sentences_noisy_lengths = []\n",
        "        for sentence in pad_sentences_noisy_batch:\n",
        "            pad_sentences_noisy_lengths.append(len(sentence))\n",
        "\n",
        "        yield pad_sentences_noisy_batch, pad_sentences_batch, pad_sentences_noisy_lengths, pad_sentences_lengths"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EeQJdKPC10Vw",
        "colab_type": "text"
      },
      "source": [
        "**Training and Validating the Model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5lNej9_Q3mB2",
        "colab_type": "text"
      },
      "source": [
        "- A session allows to execute graphs or part of graphs\n",
        "- Only after running tf.global_variables_initializer() in a session will your variables hold the values you told them to hold when you declare them.\n",
        "- tf.train.Saver() will help in saving the checkpoints. Checkpoints are nothing but the variable values at the given time.\n",
        "- Checkpoints can be saved to local file system and these checkpoints cane be restored for continuation of training,testing and prediction.\n",
        "- testing_loss_summary is an array and it will hold all the losses which are generated at the time of validation testing. This is used to compare whether the new validation loss is less than the already presented.\n",
        "- Logs are being written in the Logs folder and this folder can be read using tensorboard which will visually represent every graph element.\n",
        "- For every batch we are calling the get_batches method.\n",
        "- Every batch will call sess.run method in which the first argument will be all the resturn values which are taken from the graph. Return values can be single or can be list.\n",
        "- So in training we are getting merged, loss (Cost) and training optimizer properties so these parts of the graphs will be executed.\n",
        "- For model we are passing all the necessary inputs.\n",
        "- Looping over enumerate will give 2 outputs key and value.\n",
        "- batch_i+1 will be having the ongoing batch number.\n",
        "- If the batch_i is a multiple of display_step then we will display the progress of training to the user.\n",
        "- In the same way if batch_i is a multiple of testing_check then we start validation testing.\n",
        "- For validation we have a different set of sentence which model has no training history.\n",
        "- Validation will also happens in batches and it will calculate the loss for validation testing.\n",
        "- After this we will print 20 statements with input given to the model, prediction from the model and the orginal output.\n",
        "- For getting prediction we use sess.run in which we use prediction property to get the result.\n",
        "- For the above model input has to be in the batch wise but while prediction we pass sentence by sentence so we convert the sentence array into batch_size array. and at the end we tell run method to use the first sentence for prediction.\n",
        "- In the 20 sentences if more than 8 sentences are predicted correcly by the model then that model is saved as an accuracy model.\n",
        "- If the model loss is less than the already presented then also the model is saved.\n",
        "- If the epochs are running without improvement then we stop training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w2LNis_AHDcc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, epochs):   \n",
        "\n",
        "    with tf.Session() as sess:\n",
        "        sess.run(tf.global_variables_initializer())\n",
        "\n",
        "        saver = tf.train.Saver()\n",
        "        #checkpoint = \"/content/drive/My Drive/Ml-Models/Model-2-E-2.ckpt\"\n",
        "        #saver.restore(sess, checkpoint)\n",
        "\n",
        "        testing_loss_summary = []\n",
        "    \n",
        "        iteration = 0 # Keep track of which batch iteration is being trained\n",
        "        display_step = 30 # The progress of the training will be displayed after every 30 batches\n",
        "        stop_early = 0 \n",
        "        stop = 5 # If the batch_loss_testing does not decrease in 5 consecutive checks, stop training\n",
        "        per_epoch = 1 # Test the model 3 times per epoch\n",
        "        testing_check = (len(training_sorted)//batch_size//per_epoch)-1  # After how many batch of training validation testing has to be started.\n",
        "               \n",
        "        for epoch_i in range(1, epochs+1): \n",
        "            batch_loss = 0\n",
        "            batch_time = 0\n",
        "            is_correct = 0\n",
        "\n",
        "            print()\n",
        "            print(\"Training Model: {}\".format(epoch_i))\n",
        "\n",
        "            train_writer = tf.summary.FileWriter('./logs/1/train/{}'.format(epoch_i), sess.graph)\n",
        "            test_writer = tf.summary.FileWriter('./logs/1/test/{}'.format(epoch_i))\n",
        "\n",
        "\n",
        "             # Per batch\n",
        "            for batch_i, (input_batch, target_batch, input_length, target_length) in enumerate(get_batches(training_sorted,batch_size,threshold)):\n",
        "                start_time = time.time()\n",
        "                summary, loss, _ = sess.run([model.merged, model.cost, model.train_op],\n",
        "                                             {model.inputs: input_batch,\n",
        "                                              model.targets: target_batch,\n",
        "                                              model.inputs_length: input_length,\n",
        "                                              model.targets_length: target_length,\n",
        "                                              model.keep_prob: keep_probability})\n",
        "\n",
        "                batch_loss += loss\n",
        "                end_time = time.time()\n",
        "                batch_time += end_time - start_time\n",
        "\n",
        "                # Record the progress of training\n",
        "                train_writer.add_summary(summary, iteration)\n",
        "\n",
        "                iteration += 1\n",
        "\n",
        "                # Print info\n",
        "                if batch_i % display_step == 0 and batch_i > 0:\n",
        "                    print('Epoch {:>3}/{} Batch {:>4}/{} - Loss: {:>6.3f}, Seconds: {:>4.2f}'\n",
        "                          .format(epoch_i,\n",
        "                                  epochs, \n",
        "                                  batch_i, \n",
        "                                  len(training_sorted) // batch_size, \n",
        "                                  batch_loss / display_step, \n",
        "                                  batch_time))\n",
        "                    # Reset\n",
        "                    batch_loss = 0\n",
        "                    batch_time = 0\n",
        "\n",
        "\n",
        "                #### Run Validation Testing ####\n",
        "\n",
        "                if batch_i % testing_check == 0 and batch_i > 0:\n",
        "                    batch_loss_testing = 0\n",
        "                    batch_time_testing = 0\n",
        "\n",
        "                    for batch_i, (input_batch, target_batch, input_length, target_length) in enumerate(get_batches(validation_sorted, batch_size, threshold)):\n",
        "                        start_time_testing = time.time()\n",
        "                        is_val_correct = 0\n",
        "                        summary, loss= sess.run([model.merged, model.cost],\n",
        "                                                     {model.inputs: input_batch,\n",
        "                                                      model.targets: target_batch,\n",
        "                                                      model.inputs_length: input_length,\n",
        "                                                      model.targets_length: target_length,\n",
        "                                                      model.keep_prob: 1})                      \n",
        "\n",
        "                        batch_loss_testing += loss\n",
        "                        end_time_testing = time.time()\n",
        "                        batch_time_testing += end_time_testing - start_time_testing\n",
        "                   \n",
        "                        # Record the progress of testing\n",
        "\n",
        "                        test_writer.add_summary(summary, iteration)\n",
        "\n",
        "                    n_batches_testing = batch_i + 1\n",
        "\n",
        "                    # Print Result\n",
        "\n",
        "                    for i in range(100, 120):\n",
        "\n",
        "                        correct = validation_sorted[i]\n",
        "                        text = noise_maker(validation_sorted[i],threshold)\n",
        "                        answer_logits = sess.run(model.predictions, {model.inputs: [text]* batch_size,\n",
        "                                                                 model.inputs_length: [len(text)]* batch_size,\n",
        "                                                                 model.targets_length: [len(text)+1],\n",
        "                                                                 model.keep_prob: [1.0]})[0]\n",
        "\n",
        "\n",
        "                        correct_sentence = MakeSentenceReadable(correct)\n",
        "                        text_sentence = MakeSentenceReadable(text)\n",
        "                        answer_logits_sentence = MakeSentenceReadable(answer_logits)\n",
        "                       \n",
        "                        if (answer_logits_sentence == correct_sentence):\n",
        "                            is_correct += 1                   \n",
        "\n",
        "                        print('  Validation Input: {}'.format(text_sentence))\n",
        "                        print('  Validation Output: {}'.format(answer_logits_sentence))\n",
        "                        print('  Correct: {}'.format(correct_sentence))\n",
        "                        print('  Is Correct: {}'.format(answer_logits_sentence == correct_sentence))\n",
        "                        print()                    \n",
        "\n",
        "                    print('Testing Loss: {:>6.3f}, Seconds: {:>4.2f}'.format(batch_loss_testing / n_batches_testing, batch_time_testing))\n",
        "\n",
        "                    # If the batch_loss_testing is at a new minimum, save the model\n",
        "\n",
        "                    testing_loss_summary.append(batch_loss_testing)\n",
        "\n",
        "                    if is_correct > 8:\n",
        "                        print('New Accuracy Record!') \n",
        "                        stop_early = 0\n",
        "                        checkpoint = \"/content/drive/My Drive/Colab Notebooks/Model-{}-E-{}.ckpt\".format(is_correct,epoch_i)\n",
        "                        saver = tf.train.Saver()\n",
        "                        saver.save(sess, checkpoint)\n",
        "                    else:\n",
        "                        if batch_loss_testing <= min(testing_loss_summary):\n",
        "                            print('New Loss Record!') \n",
        "                            stop_early = 0\n",
        "                            checkpoint = \"/content/drive/My Drive/Colab Notebooks/Model-{}-E-{}.ckpt\".format(epoch_i,epoch_i)\n",
        "                            saver = tf.train.Saver()\n",
        "                            saver.save(sess, checkpoint)\n",
        "                        else:\n",
        "                            print(\"No Improvement.\")\n",
        "                            stop_early += 1\n",
        "                            if stop_early == stop:\n",
        "                                break\n",
        "\n",
        "            if stop_early == stop:\n",
        "                print(\"Stopping Training.\")\n",
        "                break\n",
        "\n",
        "\n",
        "# Train the model with the desired tuning parameters\n",
        "\n",
        "model = build_graph(keep_probability, rnn_size, num_layers, batch_size, learning_rate, embedding_size, direction)\n",
        "train(model, epochs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KjI1alhQ9Gu2",
        "colab_type": "text"
      },
      "source": [
        "**Testing the Model**\n",
        "\n",
        "- For testing we have a set of testing set. On these set every sentence is fed to the model and prediction of that sentence is taken and compared with the original output.\n",
        "- Based on the no of correct sentence we calculate accuracy. No_of_correct_sentence / total_no_of_sentences_used_for_testing.\n",
        "- After every 1000 sentences user will be notifies with the percentage of testing has completed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "neLJ0mYI2Adg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test(model,testing_set):\n",
        "    # Start session\n",
        "    with tf.Session() as sess:\n",
        "        sess.run(tf.global_variables_initializer())\n",
        "        saver = tf.train.Saver()\n",
        "\n",
        "        print()\n",
        "        print(\"Testing LSTM Model\")\n",
        "\n",
        "        testing_check = (len(testing_set)//batch_size//1)-1\n",
        "        tested = 0\n",
        "        is_correct = 0\n",
        "        #checkpoint = \"/content/drive/My Drive/Ml-Models/Model-12-E-2.ckpt\"\n",
        "        #saver.restore(sess, checkpoint)\n",
        "\n",
        "        # Per batch\n",
        "        for batch_i, (input_batch, target_batch, input_length, target_length) in enumerate(get_batches(testing_set,batch_size,threshold)):\n",
        "            if batch_i % testing_check == 0 and batch_i > 0:  \n",
        "                \n",
        "                print_tested_each = 1000\n",
        "\n",
        "                for i in range(0, len(testing_set)):\n",
        "\n",
        "                    if (tested > print_tested_each):\n",
        "                        print_tested_each  += 1000\n",
        "                        print(\"Tested {}% of test set\".format((ceil(i / len(testing_set) * 100) * 100) / 100.0))\n",
        "\n",
        "                    text = noise_maker(testing_set[i],threshold)\n",
        "                    correct = testing_set[i]\n",
        "                    answer_logits = sess.run(model.predictions, {model.inputs: [text]*batch_size,\n",
        "                                                             model.inputs_length: [len(text)]*batch_size,\n",
        "                                                             model.targets_length: [len(text)+1],\n",
        "                                                             model.keep_prob: [1.0]})[0]\n",
        "\n",
        "                    correct_sentence = MakeSentenceReadable(correct)\n",
        "                    text_sentence = MakeSentenceReadable(text)\n",
        "                    answer_logits_sentence = MakeSentenceReadable(answer_logits)\n",
        "\n",
        "                    tested += 1\n",
        "                    if (answer_logits_sentence == correct_sentence):\n",
        "                        is_correct += 1\n",
        "\n",
        "                # Reset\n",
        "                print(\"Accuracy %: {}%\".format((ceil((is_correct / tested) * 100) * 100) / 100.0))\n",
        "                print(\"Exact Accuracy: {}\".format(is_correct / tested))\n",
        "                \n",
        "                return is_correct / tested\n",
        "\n",
        "\n",
        "model = build_graph(keep_probability, rnn_size, num_layers, batch_size, learning_rate, embedding_size, direction) \n",
        "test(model, testing_sorted)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lzJFKbx2-tzE",
        "colab_type": "text"
      },
      "source": [
        "**Custom Testing**\n",
        "\n",
        "- Edit the text_sentence with the user input sentence\n",
        "- text_sentence is convrted to integer sentence as it is necessary for the input for model.\n",
        "- Restore the best available model from the checkpoint.\n",
        "- Once the predcition is returned through the model we convert the output into a human readable and printed on the console."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eqYa2Fxw-fDf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def text_to_ints(text):\n",
        "    return [vocab_to_int[word] for word in text]\n",
        "\n",
        "# Create your own sentence or use one from the dataset\n",
        "\n",
        "text_sentence = \"the first days of her existence in th country were vrey hard for dolly\"\n",
        "\n",
        "text = text_to_ints(text_sentence)\n",
        "\n",
        "checkpoint = \"/content/drive/My Drive/Ml-Models/Model-12-E-2.ckpt\"\n",
        "\n",
        "model = build_graph(keep_probability, rnn_size, num_layers, batch_size, learning_rate, embedding_size, direction) \n",
        "\n",
        "with tf.Session() as sess:\n",
        "\n",
        "    # Load saved model\n",
        "    saver = tf.train.Saver()\n",
        "    saver.restore(sess, checkpoint)\n",
        "\n",
        "    #Multiply by batch_size to match the model's input parameters\n",
        "    answer_logits = sess.run(model.predictions, {model.inputs: [text]*batch_size, \n",
        "                                                 model.inputs_length: [len(text)]*batch_size,\n",
        "                                                 model.targets_length: [len(text)+1], \n",
        "                                                 model.keep_prob: [1.0]})[0]\n",
        "\n",
        "answer_logits_sentence = MakeSentenceReadable(answer_logits)\n",
        "print(text_sentence)\n",
        "print()\n",
        "print(answer_logits_sentence)\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}